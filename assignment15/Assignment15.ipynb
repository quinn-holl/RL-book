{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- MONTE CARLO VALUE FUNCTION --------------\n",
      "dict_items([('A', 9.571428571428571), ('B', 5.642857142857143)])\n",
      "-------------- MRP VALUE FUNCTION ----------\n",
      "[12.93333333  9.6       ]\n",
      "------------- TD VALUE FUNCTION --------------\n",
      "dict_items([('A', 12.858823053430656), ('B', 8.788369448517361), ('T', 0)])\n",
      "------------- LSTD VALUE FUNCTION --------------\n",
      "[12.93279647  9.59967868]\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence, Tuple, Mapping\n",
    "import numpy as np\n",
    "from random import sample\n",
    "from collections import defaultdict\n",
    "\n",
    "S = str\n",
    "DataType = Sequence[Sequence[Tuple[S, float]]]\n",
    "ProbFunc = Mapping[S, Mapping[S, float]]\n",
    "RewardFunc = Mapping[S, float]\n",
    "ValueFunc = Mapping[S, float]\n",
    "\n",
    "\n",
    "def get_state_return_samples(\n",
    "    data: DataType\n",
    ") -> Sequence[Tuple[S, float]]:\n",
    "    \"\"\"\n",
    "    prepare sequence of (state, return) pairs.\n",
    "    Note: (state, return) pairs is not same as (state, reward) pairs.\n",
    "    \"\"\"\n",
    "    return [(s, sum(r for (_, r) in l[i:]))\n",
    "            for l in data for i, (s, _) in enumerate(l)]\n",
    "\n",
    "\n",
    "def get_mc_value_function(\n",
    "    state_return_samples: Sequence[Tuple[S, float]]\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement tabular MC Value Function compatible with the interface defined above.\n",
    "    \"\"\"\n",
    "    V = defaultdict(lambda: 0)\n",
    "    count = defaultdict(lambda: 0)\n",
    "    for s, ret in state_return_samples:\n",
    "        count[s] += 1\n",
    "        V[s] += ret\n",
    "        \n",
    "    V = {key: val/count[key] for key, val in V.items()}\n",
    "    return V.items()\n",
    "\n",
    "\n",
    "def get_state_reward_next_state_samples(\n",
    "    data: DataType\n",
    ") -> Sequence[Tuple[S, float, S]]:\n",
    "    \"\"\"\n",
    "    prepare sequence of (state, reward, next_state) triples.\n",
    "    \"\"\"\n",
    "    return [(s, r, l[i+1][0] if i < len(l) - 1 else 'T')\n",
    "            for l in data for i, (s, r) in enumerate(l)]\n",
    "\n",
    "\n",
    "def get_probability_and_reward_functions(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]]\n",
    ") -> Tuple[ProbFunc, RewardFunc]:\n",
    "    \"\"\"\n",
    "    Implement code that produces the probability transitions and the\n",
    "    reward function compatible with the interface defined above.\n",
    "    \"\"\"\n",
    "    num_transition = defaultdict(lambda: 0)\n",
    "    reward = defaultdict(lambda: 0)\n",
    "    state_freq = defaultdict(lambda: 0)\n",
    "    \n",
    "    \n",
    "    for s, r, snew in srs_samples:\n",
    "        num_transition[(s, snew)] += 1\n",
    "        reward[(s, snew)] += r\n",
    "        state_freq[s] += 1\n",
    "            \n",
    "    prob_transition = {key: (num_transition[key] / state_freq[key[0]]) \n",
    "                       for key, val in num_transition.items()} \n",
    "    \n",
    "    \n",
    "    reward_func = defaultdict(lambda: 0)\n",
    "    for key, val in prob_transition.items():\n",
    "        reward_func[key[0]] += val*(reward[key]/num_transition[key])\n",
    "        \n",
    "    \n",
    "    return prob_transition, dict(reward_func)\n",
    "    \n",
    "\n",
    "\n",
    "def get_mrp_value_function(\n",
    "    prob_func: ProbFunc,\n",
    "    reward_func: RewardFunc\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement code that calculates the MRP Value Function from the probability\n",
    "    transitions and reward function, compatible with the interface defined above.\n",
    "    Hint: Use the MRP Bellman Equation and simple linear algebra\n",
    "    \"\"\"\n",
    "    #First need to create P matrix and R vector\n",
    "    states = set(s for s in reward_func.keys())\n",
    "    m = len(states)\n",
    "    state_to_idx = dict()\n",
    "    for idx, state in enumerate(states):\n",
    "        state_to_idx[state] = idx\n",
    "\n",
    "    P = np.zeros((m,m))\n",
    "    for key, val in prob_func.items():\n",
    "        if key[1] != \"T\":\n",
    "            P[state_to_idx[key[0]]][state_to_idx[key[1]]] = val\n",
    "        \n",
    "    R = np.zeros(m)\n",
    "    for key, val in reward_func.items():\n",
    "        R[state_to_idx[key]] = val\n",
    "    \n",
    "    V = np.linalg.solve(np.eye(m) - P, R)\n",
    "    return V\n",
    "\n",
    "\n",
    "def get_td_value_function(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]],\n",
    "    num_updates: int = 300000,\n",
    "    learning_rate: float = 0.3,\n",
    "    learning_rate_decay: int = 30\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement tabular TD(0) (with experience replay) Value Function compatible\n",
    "    with the interface defined above. Let the step size (alpha) be:\n",
    "    learning_rate * (updates / learning_rate_decay + 1) ** -0.5\n",
    "    so that Robbins-Monro condition is satisfied for the sequence of step sizes.\n",
    "    \"\"\"\n",
    "    #Going to want to sample from srs_samples (how to sample from seq?)\n",
    "    \n",
    "    V = defaultdict(lambda: 0)\n",
    "    count = defaultdict(lambda: 0)\n",
    "    \n",
    "    for updates in range(num_updates):\n",
    "        s, r, snew = sample(srs_samples,1)[0]\n",
    "        count[s] += 1\n",
    "        alpha_n = learning_rate * (updates/learning_rate_decay + 1)** -0.5\n",
    "        V[s] += alpha_n*(r + V[snew] - V[s])\n",
    "        \n",
    "    return V.items()\n",
    "\n",
    "\n",
    "def get_lstd_value_function(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]]\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement LSTD Value Function compatible with the interface defined above.\n",
    "    Hint: Tabular is a special case of linear function approx where each feature\n",
    "    is an indicator variables for a corresponding state and each parameter is\n",
    "    the value function for the corresponding state.\n",
    "    \"\"\"\n",
    "    #We'll be doing updates by one \"batch\"\n",
    "    #First lets figure out the number of states, and create a dict that maps states to an \"index\" \n",
    "    states = set(s for s,r,snew in srs_samples)\n",
    "    epsilon = 1e-4\n",
    "    m = len(states)\n",
    "    state_to_idx = dict()\n",
    "    for idx, state in enumerate(states):\n",
    "        state_to_idx[state] = idx\n",
    "    \n",
    "    A_inv = np.eye(m)/epsilon\n",
    "    b = np.zeros(m)\n",
    "    for s, r, snew in srs_samples:\n",
    "        phi_i_s = np.zeros(m)\n",
    "        phi_i_s[state_to_idx[s]] = 1\n",
    "        if snew != \"T\":\n",
    "            phi_i_snew = np.zeros(m)\n",
    "            phi_i_snew[state_to_idx[snew]] = 1\n",
    "            phi2 = phi_i_s - phi_i_snew\n",
    "        else:\n",
    "            phi2 = phi_i_s\n",
    "            \n",
    "        temp = A_inv.T @ (phi2)\n",
    "        A_inv = A_inv  - np.outer(A_inv.dot(phi_i_s), temp) / (1+phi_i_s.dot(temp))\n",
    "        b += phi_i_s*r\n",
    "\n",
    "    V = A_inv @ b\n",
    "    return V\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    given_data: DataType = [\n",
    "        [('A', 2.), ('A', 6.), ('B', 1.), ('B', 2.)],\n",
    "        [('A', 3.), ('B', 2.), ('A', 4.), ('B', 2.), ('B', 0.)],\n",
    "        [('B', 3.), ('B', 6.), ('A', 1.), ('B', 1.)],\n",
    "        [('A', 0.), ('B', 2.), ('A', 4.), ('B', 4.), ('B', 2.), ('B', 3.)],\n",
    "        [('B', 8.), ('B', 2.)]\n",
    "    ]\n",
    "\n",
    "    sr_samps = get_state_return_samples(given_data)\n",
    "\n",
    "    print(\"------------- MONTE CARLO VALUE FUNCTION --------------\")\n",
    "    print(get_mc_value_function(sr_samps))\n",
    "\n",
    "    srs_samps = get_state_reward_next_state_samples(given_data)\n",
    "\n",
    "    pfunc, rfunc = get_probability_and_reward_functions(srs_samps)\n",
    "    print(\"-------------- MRP VALUE FUNCTION ----------\")\n",
    "    print(get_mrp_value_function(pfunc, rfunc))\n",
    "\n",
    "    print(\"------------- TD VALUE FUNCTION --------------\")\n",
    "    print(get_td_value_function(srs_samps))\n",
    "\n",
    "    print(\"------------- LSTD VALUE FUNCTION --------------\")\n",
    "    print(get_lstd_value_function(srs_samps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Monte Carlo value function differs from the other prediction approximation functions, which is to be expected since the Monte Carlo Value Function is the only unbiased estimator, so with a small amount of available experiences, we expect this bias for the other algorithms to be quite large, and so the other 3 VF's have much different values than MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
