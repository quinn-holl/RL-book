{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From State InventoryState(on_hand=0, on_order=0):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -10.000] with Probability 1.000\n",
      "  With Action 1:\n",
      "    To [State InventoryState(on_hand=0, on_order=1) and Reward -10.000] with Probability 1.000\n",
      "  With Action 2:\n",
      "    To [State InventoryState(on_hand=0, on_order=2) and Reward -10.000] with Probability 1.000\n",
      "From State InventoryState(on_hand=0, on_order=1):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -3.679] with Probability 0.632\n",
      "  With Action 1:\n",
      "    To [State InventoryState(on_hand=1, on_order=1) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=1) and Reward -3.679] with Probability 0.632\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=2, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -1.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=0):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -4.679] with Probability 0.632\n",
      "  With Action 1:\n",
      "    To [State InventoryState(on_hand=1, on_order=1) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=1) and Reward -4.679] with Probability 0.632\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=2, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -2.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=2, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -3.036] with Probability 0.264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from dataclasses import dataclass, replace, field\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Optional, Mapping, Dict, Tuple, TypeVar, Iterable, Sequence, Callable, Iterator, List\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from rl.function_approx import Tabular\n",
    "from rl.chapter3.simple_inventory_mdp_cap import InventoryState, SimpleInventoryMDPCap\n",
    "from rl.markov_decision_process import TransitionStep\n",
    "from rl.approximate_dynamic_programming import ValueFunctionApprox\n",
    "from rl.distribution import Choose\n",
    "from rl.iterate import last\n",
    "from rl.markov_process import MarkovProcess, NonTerminal, State\n",
    "from rl.distribution import Gamma, Distribution, Constant\n",
    "from rl.dynamic_programming import policy_iteration_result\n",
    "from rl.policy import FiniteDeterministicPolicy\n",
    "import rl.td\n",
    "import rl.monte_carlo\n",
    "import rl.td_lambda\n",
    "import rl.iterate as iterate\n",
    "from itertools import islice\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "user_gamma = 0.9\n",
    "\n",
    "si_mdp = SimpleInventoryMDPCap(capacity = user_capacity,\n",
    "                               poisson_lambda = user_poisson_lambda,\n",
    "                               holding_cost = user_holding_cost,\n",
    "                               stockout_cost = user_stockout_cost\n",
    "                              )\n",
    "\n",
    "opt_vf_vi, opt_policy_pi = policy_iteration_result(\n",
    "    si_mdp, gamma = user_gamma)\n",
    "\n",
    "print(si_mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our known optimal policy via DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.991900091403522,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.660960231637496,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.660960231637496,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.991900091403522,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991900091403522,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.89485578163003}\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pprint(opt_vf_vi)\n",
    "print(opt_policy_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets use Tabular MC Control to try and find the above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.503215118052246,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.336675438964534,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.35315787037659,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.939052659730574,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.943732795253123,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.335770099725305}\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_vf_and_policy_from_qvf(mdp, qvf):\n",
    "    opt_vf = {s: max(qvf[(s,a)] for a in mdp.actions(s))\n",
    "             for s in mdp.non_terminal_states\n",
    "             }\n",
    "    pol = {}\n",
    "    for s in mdp.non_terminal_states:\n",
    "        q_vals = {a: qvf[(s,a)] for a in mdp.actions(s)}\n",
    "        pol[s.state] = max(q_vals, key = q_vals.get)\n",
    "        \n",
    "    opt_policy = FiniteDeterministicPolicy(pol)\n",
    "    return opt_vf, opt_policy\n",
    "\n",
    "\n",
    "episode_length_tolerance = 1e-5\n",
    "eps_as_func_of_episodes = lambda k : k ** -1\n",
    "\n",
    "initial_qvf_dict = {(s,a): 0. for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)}\n",
    "qvfs = rl.monte_carlo.glie_mc_tabular_control(si_mdp, Choose(si_mdp.non_terminal_states),\n",
    "                               initial_qvf_dict, user_gamma,\n",
    "                               eps_as_func_of_episodes, \n",
    "                               episode_length_tolerance)\n",
    "\n",
    "\n",
    "num_episodes = 10000\n",
    "final_qvf = iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "opt_vf, opt_policy = get_vf_and_policy_from_qvf(si_mdp, final_qvf)\n",
    "pprint(opt_vf)\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our answers are reasonable when we compare them to the DP solution above, with each value function within $2\\%$. However, it seems that the optimal action differs for one state, the (0,0) state. The fact that the optimal value functions don't differ that much seem to tell me that the optimal policy choice of Action 1 over Action 2 leads to very negligible differences in the reward structure, and since Tabular MC Control doesn't have the best convergence properties, that we've reached a \"good enough\" optimal policy, given the facts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And now Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.57887962360952,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.16224382407824,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.693013053803757,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.35708328317616,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.329398206917812,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.579792381783722}\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_episode_length = 100\n",
    "epsilon = 0.2\n",
    "qvfs = rl.td.tabular_q_learning(si_mdp, \n",
    "                                policy_from_q= lambda f, m : rl.monte_carlo.epsilon_greedy_policy_tab(\n",
    "                                    q = f,\n",
    "                                    mdp = si_mdp,\n",
    "                                    epsilon = epsilon),\n",
    "                                states = Choose(si_mdp.non_terminal_states),\n",
    "                                approx_0 = initial_qvf_dict,\n",
    "                                gamma = user_gamma,\n",
    "                                max_episode_length = max_episode_length\n",
    "                               )\n",
    "final_qvf_learn = iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "opt_vf, opt_policy = get_vf_and_policy_from_qvf(si_mdp,final_qvf_learn)\n",
    "\n",
    "\n",
    "pprint(opt_vf)\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test these functions on the OVF/OP obtained by ADP on AssetAllocDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.chapter7.asset_alloc_discrete import AssetAllocDiscrete\n",
    "from rl.distribution import Gaussian\n",
    "from rl.function_approx import DNNSpec, DNNApprox\n",
    "\n",
    "steps = 4\n",
    "mu = 0.13\n",
    "sig = 0.2\n",
    "r = 0.07\n",
    "a = 1.0\n",
    "init_wealth = 1.0\n",
    "init_wealth_stdev = 0.1\n",
    "\n",
    "excess = mu - r\n",
    "var = sig**2\n",
    "base_alloc = excess / (a*var)\n",
    "\n",
    "\n",
    "risky_ret = [Gaussian(μ = mu, σ = sig) for _ in range(steps)]\n",
    "riskless_ret = [r for _ in range(steps)]\n",
    "utility_function = lambda x: -np.exp(-a*x) / a\n",
    "alloc_choices = np.linspace(2/3 * base_alloc, 4/3 * base_alloc, 11)\n",
    "\n",
    "feature_funcs = [\n",
    "    lambda _: 1,\n",
    "    lambda w_x: w_x[0],\n",
    "    lambda w_x: w_x[1],\n",
    "    lambda w_x: w_x[1] * w_x[1]\n",
    "]\n",
    "\n",
    "\n",
    "dnn = DNNSpec(\n",
    "    neurons = [],\n",
    "    bias = False,\n",
    "    hidden_activation = lambda x:x,\n",
    "    hidden_activation_deriv = lambda y: np.ones_like(y),\n",
    "    output_activation = lambda x: -np.sign(a) * np.exp(-x),\n",
    "    output_activation_deriv = lambda y: -y\n",
    ")\n",
    "\n",
    "init_wealth_distr = Gaussian(μ = init_wealth, σ = init_wealth_stdev)\n",
    "aad = AssetAllocDiscrete(\n",
    "    risky_return_distributions = risky_ret,\n",
    "    riskless_returns=riskless_ret,\n",
    "    utility_func=utility_function,\n",
    "    risky_alloc_choices=alloc_choices,\n",
    "    feature_functions=feature_funcs,\n",
    "    dnn_spec=dnn,\n",
    "    initial_wealth_distribution=init_wealth_distr\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_length_tolerance = 1e-5\n",
    "eps_as_func_of_episodes = lambda k : k ** -1\n",
    "\n",
    "initial_qvf_dict = \n",
    "\n",
    "\n",
    "\n",
    "aad_mdp = aad.get_mdp(0)\n",
    "\n",
    "\n",
    "aad_qvfs = rl.monte_carlo.glie_mc_tabular_control(aad_mdp, Choose(aad_mdp.non_terminal_states),\n",
    "                               initial_qvf, user_gamma,\n",
    "                               eps_as_func_of_episodes, \n",
    "                               episode_length_tolerance)\n",
    "\n",
    "\n",
    "num_episodes = 10000\n",
    "aad_final_qvf = iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "aad_opt_vf, aad_opt_policy = get_vf_and_policy_from_qvf(aad_mdp, aad_final_qvf)\n",
    "pprint(aad_opt_vf)\n",
    "print(aad_opt_policy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
