{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.dynamic_programming import V, S, A\n",
    "from rl import dynamic_programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl import markov_process, markov_decision_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Mapping, Iterator, TypeVar, Tuple, Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import Categorical, Choose\n",
    "from rl.iterate import converged, iterate\n",
    "from rl.markov_process import NonTerminal, State, Terminal\n",
    "from rl.markov_decision_process import (FiniteMarkovDecisionProcess,\n",
    "                                        FiniteMarkovRewardProcess)\n",
    "from rl.policy import FinitePolicy, FiniteDeterministicPolicy\n",
    "from rl.dynamic_programming import extended_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.midterm_2022.priority_q import  PriorityQueue\n",
    "from rl.midterm_2022 import grid_maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will implement the gaps-based value iteration algorithm mentioned in class.\n",
    "\n",
    "The gaps-based iteration algorithm proceeds as follows\n",
    "\n",
    "1. Initialize the value function to zero for all states: $v[s] = 0\\ \\forall s \\in \\mathcal{N}$\n",
    "2. Calculate the gaps for each state: $g[s] = |v[s] - \\max_a \\mathcal{R}(s,a) + \\sum_{s'} \\mathcal{P}(s,a,s') \\cdot v(s')|$\n",
    "3. While there is some gap that exceeds a threshold\n",
    " - Select the state with the  largest gap: $s_{max} = \\arg\\max_{s \\in \\mathcal{N}} g[s]$\n",
    " - Update the value function for $s_{max}$: $v[s_{max}] = \\max_a \\mathcal{R}(s_{max},a) + \\sum_{s'}\\mathcal{P}(s_{max},a,s') \\cdot v(s')$\n",
    " -  Update the gap for $s_{max}$: $g[s_{max}] = 0$\n",
    "4. Return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test your implementation on a grid maze MDP. We have defined this class in \"grid_maze.py\", you should  briefly familiarize yourself with that code. In particular pay attention to the difference in reward functions for the two classes \"GridMazeMDP_Dense\" and \"GridMazeMDP_Sparse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can use the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_maze = grid_maze.Maze(10, 10)\n",
    "maze_mdp = grid_maze.GridMazeMDP_Sparse(underlying_maze, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can visualize the maze if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "|*  | | |       |   |\n",
      "| + + + + + +-+-+-+ +\n",
      "| |       |       | |\n",
      "| + +-+-+-+ + +-+-+ +\n",
      "| |   | | | | |     |\n",
      "| +-+ + + +-+-+-+ + +\n",
      "| |               | |\n",
      "| + +-+ +-+ + +-+ +-+\n",
      "| | |   | | | | |   |\n",
      "|-+ +-+ + + + + +-+-+\n",
      "|     |   | |   |   |\n",
      "|-+-+ +-+-+ +-+ + +-+\n",
      "|   |   |   |       |\n",
      "|-+ + +-+ + +-+ +-+-+\n",
      "|       | |   |   | |\n",
      "| + + +-+-+ + + + + +\n",
      "| | |     | | | |   |\n",
      "|-+ + + + + +-+ + +-+\n",
      "|   | | | | |   |   |\n",
      "|-+-+-+-+-+-+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "print(maze_mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also visualize a policy on the mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "|* <|v|v|v < < <|> v|\n",
      "| + + + + + +-+-+-+ +\n",
      "|^|^ < < <|^ < < <|v|\n",
      "| + +-+-+-+ + +-+-+ +\n",
      "|^|^ <|v|v|^|^|> v <|\n",
      "| +-+ + + +-+-+-+ + +\n",
      "|^|> ^ < < < < < <|^|\n",
      "| + +-+ +-+ + +-+ +-+\n",
      "|^|^|> ^|v|^|^|v|^ <|\n",
      "|-+ +-+ + + + + +-+-+\n",
      "|> ^ <|^ <|^|^ <|v <|\n",
      "|-+-+ +-+-+ +-+ + +-+\n",
      "|> v|^ <|> ^|> ^ < <|\n",
      "|-+ + +-+ + +-+ +-+-+\n",
      "|> > ^ <|^|^ <|^ <|v|\n",
      "| + + +-+-+ + + + + +\n",
      "|^|^|^ < <|^|^|^|^ <|\n",
      "|-+ + + + + +-+ + +-+\n",
      "|> ^|^|^|^|^|> ^|^ <|\n",
      "|-+-+-+-+-+-+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 0.9)\n",
    "print(maze_mdp.print_policy(v2_res[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to make use of the PriorityQueue class in your implementation. A PriorityQueue is an ordered queue which supports the following operations\n",
    "1. isEmpty(self): check if the queue is empty   \n",
    "2. contains(self, element): check if the queue contains an element\n",
    "3. peek(self): peek at the highest priority element in the queue    \n",
    "4. pop(self): remove and return the highest priority element in the queue    \n",
    "5. insert(self, element, priority): insert an element into the queue with given priority\n",
    "6. update(self, element, new_priority): update the priority of an element in the queue\n",
    "7. delete(self, element): delete an element from the queue\n",
    "\n",
    "Below are some examples of using the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True : the queue is empty\n",
      "False : the queue is not empty\n",
      "True : the queue contains a\n",
      "False : the queue does not contain a\n",
      "(1, 'a') : a is the first element in the queue\n",
      "True : the queue now contains b\n",
      "(0, 'b') : b is now at the front of the queue\n",
      "b : we removed b from the queue\n",
      "False : the queue still nonempty\n",
      "True : the queue still contains a\n",
      "False : the queue does not contain b anymore\n",
      "(1, 'a') : a is at the front of the queue\n",
      "(1, 'a') : a is still at the front of the queue\n",
      "(5, 'c') : after updating a is no longer at the front of the queue\n"
     ]
    }
   ],
   "source": [
    "q: PriorityQueue = PriorityQueue()\n",
    "print(q.isEmpty(), ':', \"the queue is empty\")\n",
    "q.insert(\"a\", 1)\n",
    "print(q.isEmpty(), ':',  \"the queue is not empty\")\n",
    "print(q.contains(\"a\"), ':',  \"the queue contains a\")\n",
    "print(q.contains(\"b\"), ':',  \"the queue does not contain a\")\n",
    "print(q.peek(), ':',  \"a is the first element in the queue\")\n",
    "q.insert(\"b\", 0)\n",
    "print(q.contains(\"b\"), ':',  \"the queue now contains b\")\n",
    "print(q.peek(), ':',  \"b is now at the front of the queue\")\n",
    "x = q.pop()\n",
    "print(x, ':',  \"we removed b from the queue\")\n",
    "print(q.isEmpty(), ':',  \"the queue still nonempty\")\n",
    "print(q.contains(\"a\"), ':',  \"the queue still contains a\")\n",
    "print(q.contains(\"b\"), ':',  \"the queue does not contain b anymore\")\n",
    "print(q.peek(), ':',  \"a is at the front of the queue\")\n",
    "q.insert(\"c\", 5)\n",
    "print(q.peek(), ':',  \"a is still at the front of the queue\")\n",
    "q.update(\"a\", 6)\n",
    "print(q.peek(), ':',  \"after updating a is no longer at the front of the queue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def invert_transition_map(mdp: markov_decision_process.FiniteMarkovDecisionProcess[S, A]) ->\\\n",
    "            Mapping[S, Iterable[S]]:\n",
    "    '''\n",
    "    YOUR CODE HERE\n",
    "    Implement the invert_transition_map method\n",
    "    Should essentially create a dictionary with each state as a key and value of other states that this state's\n",
    "    value function depends on\n",
    "    '''\n",
    "    inverted_mapping = dict()\n",
    "    \n",
    "    #iterate over states, first get complete key dictionary\n",
    "    for s, _ in mdp.mapping.items():\n",
    "        if isinstance(s, Terminal):\n",
    "            pass\n",
    "        else:\n",
    "            inverted_mapping[s] = list()\n",
    "    \n",
    "    \n",
    "    #Now iterate and fill dictionary\n",
    "    for s, d in mdp.mapping.items():\n",
    "        if isinstance(s, NonTerminal):\n",
    "            for a, d1 in d.items():\n",
    "                for (s1,r), p in d1:\n",
    "                    if isinstance(s1, Terminal):\n",
    "                        pass\n",
    "                    else:\n",
    "                        inverted_mapping[s1].append(s)    \n",
    "\n",
    "    return inverted_mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert(x: float):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/x\n",
    "    \n",
    "epsilon = 1e-5\n",
    "\n",
    "\n",
    "def gaps_value_iteration(\n",
    "    mdp: markov_decision_process.FiniteMarkovDecisionProcess[S, A],\n",
    "    gamma: float, \n",
    "    gaps: PriorityQueue) -> Iterator[V[S]]:\n",
    "    '''\n",
    "    Calculate the value function (V*) of the given MDP by applying the\n",
    "    update function repeatedly until the values converge.\n",
    "\n",
    "    '''\n",
    "    dependency_map = invert_transition_map(mdp)\n",
    "    v_0: V[S] = {s: 0.0 for s in mdp.non_terminal_states}\n",
    "    \n",
    "        \n",
    "    def update(v: V[S]) -> V[S]:\n",
    "        '''\n",
    "        YOUR CODE HERE\n",
    "        perform a single update to v for the state with the largest gap\n",
    "        update the gaps for any dependent states\n",
    "        '''\n",
    "        #Get element with largest gap\n",
    "        largestGapState = gaps.pop()\n",
    "        \n",
    "        #update its value function and save initial\n",
    "        v[largestGapState] = max(mdp.mapping[largestGapState][a].expectation(\n",
    "                                lambda s_r: s_r[1] + gamma*extended_vf(v,s_r[0])\n",
    "                                ) for a in mdp.actions(largestGapState))\n",
    "        \n",
    "        \n",
    "        #update gaps for each state it depends on \n",
    "        for s1 in dependency_map[largestGapState]:\n",
    "            newVal = max(mdp.mapping[s1][a].expectation(\n",
    "                          lambda s_r: s_r[1] + gamma*extended_vf(v,s_r[0])\n",
    "                    ) for a in mdp.actions(s1))\n",
    "            newgap = abs(v[s1] - newVal)\n",
    "            if(newgap < epsilon):\n",
    "                pass\n",
    "            else:\n",
    "                if(gaps.contains(s1)):\n",
    "                    gaps.update(s1,invert(newgap))\n",
    "                else:\n",
    "                    gaps.insert(s1,invert(newgap))\n",
    "    \n",
    "        return v\n",
    "    \n",
    "    \n",
    "    return iterate(update, v_0)\n",
    "\n",
    "\n",
    "def gaps_value_iteration_result(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    gamma: float\n",
    ") -> Tuple[V[S], FiniteDeterministicPolicy[S, A]]:\n",
    "    \n",
    "    gaps = PriorityQueue()\n",
    "    \n",
    "    '''\n",
    "    YOUR CODE HERE\n",
    "    instantiate the value function and populate the gaps\n",
    "    ''' \n",
    "    v_0: V[S] = {s: 0.0 for s in mdp.non_terminal_states}\n",
    "    \n",
    "    \n",
    "    #now fill in gaps, but only for states with nonzero reward\n",
    "    for s, d in mdp.mapping.items():\n",
    "        if isinstance(s, Terminal):\n",
    "            pass\n",
    "        else:\n",
    "            #first calculate gaps and see if nonzero\n",
    "            reward = max(mdp.mapping[s][a].expectation(\n",
    "                            lambda s_r: s_r[1] + gamma*extended_vf(v_0,s_r[0])\n",
    "                            ) for a in mdp.actions(s))\n",
    "            if(abs(reward) > epsilon):\n",
    "                gaps.insert(s, invert(abs(reward)))\n",
    "                \n",
    "    \n",
    "    \n",
    "    ''' END YOUR CODE ''' \n",
    "    \n",
    "    def criterion(x,y):\n",
    "        '''\n",
    "        YOUR CODE HERE\n",
    "        implement the criterion for convergence of the value function \n",
    "        ''' \n",
    "        #I'd rather be able to just call max(x.gaps.peek()[0], y.gaps.peek()[1]) but don't have access\n",
    "        tolerance = 1e-5\n",
    "        if(gaps.isEmpty()):\n",
    "            return True\n",
    "        \n",
    "        return gaps.peek()[0] < tolerance\n",
    "        \n",
    "        ''' END YOUR CODE ''' \n",
    "        \n",
    "    opt_vf: V[S] = converged(\n",
    "        gaps_value_iteration(mdp, gamma, gaps),\n",
    "        done= criterion \n",
    "    )\n",
    "        \n",
    "    opt_policy: markov_decision_process.FiniteDeterministicPolicy[S, A] = dynamic_programming.greedy_policy_from_vf(\n",
    "        mdp,\n",
    "        opt_vf,\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    return opt_vf, opt_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not change the code below here, just run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the VF for a maze with sparse rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_maze = grid_maze.Maze(50, 50)\n",
    "maze_mdp = grid_maze.GridMazeMDP_Sparse(underlying_maze, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printing the runtime for the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15527701377868652\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v1_res = gaps_value_iteration_result(maze_mdp, 0.9)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9072070121765137\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 0.9)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confirming that the value functions are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert v1_res[1] == v2_res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the VF for a maze with dense rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_mdp = grid_maze.GridMazeMDP_Dense(underlying_maze, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printing the runtime for the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9927589893341064\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v1_res = gaps_value_iteration_result(maze_mdp, 1)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.92085599899292\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 1)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confirming that the value functions are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert v1_res[1] == v2_res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the convergence time for gaps iteration is about an entire order of magnitude faster than plain value_iteration for the sparse MDP, while the gaps_iteration is significantly slower for the dense_MDP. This later result can be seen simply from how we first add states to our Priority Queue. For the sparse MDP, our reward function only gives non-zero values for the states that can reach the goal, so roughly 1,2,or 3 states, and thus our PQ is smaller for a longer time than for the Dense MDP, making our insert, update, and pop operations quicker. Also, the speedup attributable to the sparse_MDP is because we have a few significant states at a time that will impact other state's value functions a lot more than others. In the dense_MDP case this is not true, i.e. the rewards (and thus the intial value functions) all have a similar effect on each dependent state, meaning that there isn't one step that really has the most impact on the next iteration of the value function. As for the differences of convergence times between the gap and non-gap iteration method for the sparse_MDP, that simply comes from the fact that we update the states which have a larger impact on convergence more times relative to not as important states, meaning we need less total iterations to get to our convergence tolerance quicker. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
