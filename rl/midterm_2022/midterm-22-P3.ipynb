{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "\n",
    "Part 1: $\\textbf{With proper mathematical notation, model this as a finite MDP specifying the states, actions, rewards, state-transition probabilities and discount factor.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, our states can be expressed as the state of the die on the table in sorted ascending order, and then a pair of values corresponding to the number of 1's in our hand and the sum of the die in our hand\n",
    "\n",
    "\\begin{equation*} \\mathcal{S} = \\{ [i_{1}, ..., i_{l}] , (I_{1}, \\sum_{k=l+1}^{N} i_{k}) \\} \\\\\n",
    "    0 \\leq l \\leq N  \\\\\n",
    "    1 \\leq i_{p} \\leq K \\\\\n",
    "    i_{k-1} \\leq i_{k} \\leq i_{k+1}\n",
    "\\end{equation*}\n",
    "\n",
    "and we can also illustrate the set of Terminal states corresponding to no die on the table:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathcal{T} = \\{ [], (I_{1}, \\sum_{k=1}^{N} i_{k}) \\}\n",
    "\\end{equation*}\n",
    "\n",
    "Our action space is thus defined as a set of indices that take the die from the table (i.e. the first list in the state), and transferring it to the hand, thus crystallizing the die values. We must also state that the action must have a list of size greater than 1:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathcal{A} = \\{ j_{1}, ..., j_{q} \\}  \\\\\n",
    "    1 \\leq q \\leq l \\\\\n",
    "    j_{k} \\in \\{0,1\\}\n",
    "\\end{equation*}\n",
    "\n",
    "Also, our rewards functions are zero for the nonterminal states, so we will show only the rewards for the terminal states $t \\in \\mathcal{T} $\n",
    "\n",
    "\\begin{equation*}\n",
    "    R(t) = \n",
    "    \\begin{cases}\n",
    "        0, & \\sum_{k=1}^{k=N} I(i_{k}) \\le C  \\\\\n",
    "        \\sum_{k=1}^{k=N} i_{k}, & otherwise\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "where $I(i_{k})$ is the indicator function for whether the die is a 1 or not.\n",
    "\n",
    "\n",
    "The transition probabilities vary depending on the action taken by our agent, and are shown below:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathcal{P}(s,a,s^{'}) = (\\frac{1}{K})^{N-\\pi(a)} \\cdot c_{1} \\\\\n",
    "    \\pi(a) = \\textrm{number of die left on table after action a} \\\\\n",
    "    c_{1} = \\textrm{Number of ways we could have gotten to table die}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: $\\textbf{Implement this MDP in python}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Mapping, Dict, Tuple, List\n",
    "from rl import markov_process\n",
    "from rl.distribution import Categorical, Constant, FiniteDistribution\n",
    "from rl.markov_process import FiniteMarkovProcess, NonTerminal, MarkovRewardProcess, FiniteMarkovRewardProcess\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.policy import FiniteDeterministicPolicy\n",
    "from rl.dynamic_programming import value_iteration_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DieGame:\n",
    "    table: Tuple\n",
    "    hand: Tuple\n",
    "                \n",
    "\n",
    "ActionMapping = Mapping[DieGame, Mapping[List, Categorical[Tuple[DieGame, float]]]]\n",
    "        \n",
    "        \n",
    "        \n",
    "class die_MDP(FiniteMarkovDecisionProcess[DieGame, List]):\n",
    "    sides: int\n",
    "    die: int\n",
    "    cutoff: int\n",
    "    \n",
    "    def __init__(self, K: int, N: int, C: int):\n",
    "        self.sides = K\n",
    "        self.die = N\n",
    "        self.cutoff = C\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "        \n",
    "    def get_hand_state(self, die_list: Tuple):\n",
    "        ones = [x for x in die_list if x == 1]\n",
    "        return (len(ones), sum(die_list))\n",
    "        \n",
    "    def get_unique_hand_states(self, hand_list: List[Tuple]):\n",
    "        unique_states = dict()\n",
    "        for hl in hand_list:\n",
    "            hand_state = self.get_hand_state(hl)\n",
    "            if hand_state in unique_states:\n",
    "                unique_states[hand_state] += 1\n",
    "            else:\n",
    "                unique_states[hand_state] = 1\n",
    "        return unique_states\n",
    "    \n",
    "    def get_unique_table_states(self, table_list: List[Tuple]):\n",
    "        unique_states = dict()\n",
    "        for tl in table_list:\n",
    "            table_state = self.get_table_state(tl)\n",
    "            if table_state in unique_states:\n",
    "                unique_states[table_state] += 1\n",
    "            else:\n",
    "                unique_states[table_state] = 1\n",
    "        return unique_states\n",
    "    \n",
    "    def get_table_state(self, die_list: Tuple):\n",
    "        sorted_state = sorted(list(die_list))\n",
    "        sorted_state = tuple(sorted_state)\n",
    "        return sorted_state\n",
    "            \n",
    "        \n",
    "    def reward_function(self, die_list: Tuple):\n",
    "        if die_list[0] >= C:\n",
    "            return die_list[1]\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def delete_multiple_element(self, list_object, indices):\n",
    "        indices = sorted(indices, reverse = True)\n",
    "        for idx in indices:\n",
    "            if idx < len(list_object):\n",
    "                list_object.pop(idx)\n",
    "        \n",
    "        \n",
    "    def get_action_transition_reward_map(self) -> ActionMapping:\n",
    "        d: Dict[DieGame, Dict[List, Categorical[Tuple[DieGame, float]]]] = {}\n",
    "        splits = list(itertools.product(range(N+1), repeat = 2))\n",
    "        splits = [x for x in splits if sum(x) == N and x[0] != 0]\n",
    "        \n",
    "        for split in splits:\n",
    "\n",
    "            table_list = list(itertools.product(range(1,K+1), repeat = split[0]))\n",
    "            hand_list = list(itertools.product(range(1,K+1), repeat = split[1]))\n",
    "            \n",
    "            #Idea: lets get unique state possibilities from hand_list to lessen for loop load\n",
    "            hand_state = self.get_unique_hand_states(hand_list)\n",
    "            table_state = self.get_unique_table_states(table_list)\n",
    "            print(split)\n",
    "            \n",
    "            for tl, _ in table_state.items():\n",
    "                for hl, _ in hand_state.items():    \n",
    "                        \n",
    "                    state = DieGame(tl, hl)\n",
    "            \n",
    "                    d1: Dict[List, Categorical[Tuple[DieGame, float]]] = {}\n",
    "                    \n",
    "                    actions = list(itertools.product([0,1], repeat = len(tl)))\n",
    "                    actions = [x for x in actions if sum(x) >= 1]\n",
    "                    #need to remove action where we don't move any die\n",
    "                    \n",
    "                    for action in actions:\n",
    "                        state_prob_map: Mapping[Tuple[DieGame, float], float] = {}\n",
    "                            \n",
    "                        #For this action first swap die to hand, then throw die again\n",
    "                        tl_a = list(tl)\n",
    "                        hl_a = list(hl)\n",
    "                        swaplist = list()\n",
    "                        for idx, swap in enumerate(action):\n",
    "                            if swap == 1:\n",
    "                                val = tl_a[idx]\n",
    "                                swaplist.append(idx)\n",
    "                                hl_a[1] += val\n",
    "                                if val == 1:\n",
    "                                    hl_a[0] += 1\n",
    "                                \n",
    "                        self.delete_multiple_element(tl_a, swaplist)\n",
    "                        hl_a = tuple(hl_a)\n",
    "                        tl_a = self.get_table_state(tl_a)\n",
    "                                \n",
    "                        #First check that new table list is not empty, otherwise calc reward\n",
    "                        if len(tl_a) == 0:\n",
    "                            reward = self.reward_function(hl_a)\n",
    "                            state_prob_map[(DieGame(tl_a,hl_a), reward)] = 1\n",
    "                        else:                            \n",
    "                            \n",
    "                            new_table_list = list(itertools.product(range(1,K+1), repeat = len(tl_a)))\n",
    "                            new_table_states = self.get_unique_table_states(new_table_list)\n",
    "                            for new_state_table, count in new_table_states.items():\n",
    "                                state_prob_map[(DieGame(new_state_table, hl_a), 0)] = \\\n",
    "                                                pow((1./K), len(new_state_table))*count\n",
    "                        \n",
    "                        d1[action] = Categorical(state_prob_map)\n",
    "                    \n",
    "                    \n",
    "                    d[state] = d1\n",
    "        return d\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "(2, 4)\n",
      "(3, 3)\n",
      "(4, 2)\n",
      "(5, 1)\n",
      "(6, 0)\n"
     ]
    }
   ],
   "source": [
    "N = 6\n",
    "K = 4\n",
    "C = 2\n",
    "user_gamma = 1.0\n",
    "\n",
    "die_mdp = die_MDP(K = K, N = N, C = C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_vf_vi, opt_policy_vi = value_iteration_result(die_mdp, gamma = user_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Expected Score of the game playing optimally:\n",
    " This is equivalent to asking what is the average value of the optimal value function when our state is at the start, i.e. an empty hand. To do this, we must sum up the product of the initial state and its value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Score of game:  15.216421169450586\n"
     ]
    }
   ],
   "source": [
    "#To get all of the inital starting states, use our itertools t vals\n",
    "table_list = list(itertools.product(range(1,K+1), repeat = N))\n",
    "table_states = die_mdp.get_unique_table_states(table_list)\n",
    "\n",
    "\n",
    "expected_score = 0\n",
    "hl = (0,0)\n",
    "for tl, _ in table_states.items():\n",
    "    starting_state = NonTerminal(DieGame(tl, hl))\n",
    "    expected_score += (1.0/len(table_states))*opt_vf_vi[starting_state]\n",
    "\n",
    "print(\"Expected Score of game: \",expected_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the optimal action when rolling {1,2,2,3,3,4} on the first roll\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(opt_policy_vi)\n",
    "#IF i print this to screen, pdf will be unreadable, please run it to check my answer that i give below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, as we can see, our optimal policy for the state corresponding to (1,2,2,3,3,4) and none in the hand is the action: (1,0,0,0,0,0). In other words, it means that we should choose to transfer the 1 to our hand and nothing else. This makes sense, as we are ensuring that we have a good chance to get the number of required ones, and we chose not to pocket the highest roll we can possibly get, but that's because we are constricting ourself and making it harder to reach the required amount of 1's for our setup, which was 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
